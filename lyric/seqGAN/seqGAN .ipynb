{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "009_seqGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg7emNDaAmgI"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt4L0Tq0ArkN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import collections\n",
        "import pickle\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import math\n",
        "import copy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjsKLvmfBFrU"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bOAlx6PJWa6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkPJioB8gqfk"
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIHYtPMe_cGY"
      },
      "source": [
        "## 1. Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6deYvocc_iQD"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\" Generator \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, use_cuda):\n",
        "        super(Generator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.use_cuda = use_cuda\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "        self.init_params()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Embeds input and applies LSTM on the input sequence.\n",
        "        Inputs: x\n",
        "            - x: (batch_size, seq_len), sequence of tokens generated by generator\n",
        "        Outputs: out\n",
        "            - out: (batch_size * seq_len, vocab_size), lstm output prediction\n",
        "        \"\"\"\n",
        "        self.lstm.flatten_parameters()\n",
        "        h0, c0 = self.init_hidden(x.size(0))\n",
        "        emb = self.embed(x) # batch_size * seq_len * emb_dim \n",
        "        out, _ = self.lstm(emb, (h0, c0)) # out: batch_size * seq_len * hidden_dim\n",
        "        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # (batch_size*seq_len) * vocab_size\n",
        "        return out\n",
        "\n",
        "    def step(self, x, h, c):\n",
        "        \"\"\"\n",
        "        Embeds input and applies LSTM one token at a time (seq_len = 1).\n",
        "        Inputs: x, h, c\n",
        "            - x: (batch_size, 1), sequence of tokens generated by generator\n",
        "            - h: (1, batch_size, hidden_dim), lstm hidden state\n",
        "            - c: (1, batch_size, hidden_dim), lstm cell state\n",
        "        Outputs: out, h, c\n",
        "            - out: (batch_size, vocab_size), lstm output prediction\n",
        "            - h: (1, batch_size, hidden_dim), lstm hidden state\n",
        "            - c: (1, batch_size, hidden_dim), lstm cell state \n",
        "        \"\"\"\n",
        "        self.lstm.flatten_parameters()\n",
        "        emb = self.embed(x) # batch_size * 1 * emb_dim\n",
        "        out, (h, c) = self.lstm(emb, (h, c)) # out: batch_size * 1 * hidden_dim\n",
        "        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # batch_size * vocab_size\n",
        "        return out, h, c\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h = torch.zeros(1, batch_size, self.hidden_dim)\n",
        "        c = torch.zeros(1, batch_size, self.hidden_dim)\n",
        "        if self.use_cuda:\n",
        "            h, c = h.cuda(), c.cuda()\n",
        "        return h, c\n",
        "    \n",
        "    def init_params(self):\n",
        "        for param in self.parameters():\n",
        "            param.data.uniform_(-0.05, 0.05)\n",
        "\n",
        "    def sample(self, batch_size, seq_len, x=None):\n",
        "        \"\"\"\n",
        "        Samples the network and returns a batch of samples of length seq_len.\n",
        "        Outputs: out\n",
        "            - out: (batch_size * seq_len)\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        if x is None:\n",
        "            h, c = self.init_hidden(batch_size)\n",
        "            x = torch.zeros(batch_size, 1, dtype=torch.int64)\n",
        "            if self.use_cuda:\n",
        "                x = x.cuda()\n",
        "            for _ in range(seq_len):\n",
        "                out, h, c = self.step(x, h, c)\n",
        "                prob = torch.exp(out)\n",
        "                x = torch.multinomial(prob, 1)\n",
        "                samples.append(x)\n",
        "        else:\n",
        "            h, c = self.init_hidden(x.size(0))\n",
        "            given_len = x.size(1)\n",
        "            lis = x.chunk(x.size(1), dim=1)\n",
        "            for i in range(given_len):\n",
        "                out, h, c = self.step(lis[i], h, c)\n",
        "                samples.append(lis[i])\n",
        "            prob = torch.exp(out)\n",
        "            x = torch.multinomial(prob, 1)\n",
        "            for _ in range(given_len, seq_len):\n",
        "                samples.append(x)\n",
        "                out, h, c = self.step(x, h, c)\n",
        "                prob = torch.exp(out)\n",
        "                x = torch.multinomial(prob, 1)\n",
        "        out = torch.cat(samples, dim=1) # along the batch_size dimension\n",
        "        return out"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cdRt324E7TA"
      },
      "source": [
        "## 2. Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKvcwbEZE-CO"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    A CNN for text classification.\n",
        "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
        "    Highway architecture based on the pooled feature maps is added. Dropout is adopted.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, vocab_size, embedding_dim, filter_sizes, num_filters, dropout_prob):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, num_f, (f_size, embedding_dim)) for f_size, num_f in zip(filter_sizes, num_filters)\n",
        "        ])\n",
        "        self.highway = nn.Linear(sum(num_filters), sum(num_filters))\n",
        "        self.dropout = nn.Dropout(p = dropout_prob)\n",
        "        self.fc = nn.Linear(sum(num_filters), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Inputs: x\n",
        "            - x: (batch_size, seq_len)\n",
        "        Outputs: out\n",
        "            - out: (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        emb = self.embed(x).unsqueeze(1) # batch_size, 1 * seq_len * emb_dim\n",
        "        convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs] # [batch_size * num_filter * seq_len]\n",
        "        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs] # [batch_size * num_filter]\n",
        "        out = torch.cat(pools, 1)  # batch_size * sum(num_filters)\n",
        "        highway = self.highway(out)\n",
        "        transform = F.sigmoid(highway)\n",
        "        out = transform * F.relu(highway) + (1. - transform) * out # sets C = 1 - T\n",
        "        out = F.log_softmax(self.fc(self.dropout(out)), dim=1) # batch * num_classes\n",
        "        return out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31k0Dp6ZRap-"
      },
      "source": [
        "## 3. Target LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90mCsVA3RccD"
      },
      "source": [
        "class TargetLSTM(nn.Module):\n",
        "    \"\"\" Target LSTM \"\"\"\n",
        "\n",
        "    def __init__(self,  vocab_size, embedding_dim, hidden_dim, use_cuda):\n",
        "        super(TargetLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.use_cuda = use_cuda\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "        self.init_params()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Embeds input and applies LSTM on the input sequence.\n",
        "        Inputs: x\n",
        "            - x: (batch_size, seq_len), sequence of tokens generated by generator\n",
        "        Outputs: out\n",
        "            - out: (batch_size, vocab_size), lstm output prediction\n",
        "        \"\"\"\n",
        "        self.lstm.flatten_parameters()\n",
        "        h0, c0 = self.init_hidden(x.size(0))\n",
        "        emb = self.embed(x) # batch_size * seq_len * emb_dim \n",
        "        out, _ = self.lstm(emb, (h0, c0)) # out: seq_len * batch_size * hidden_dim\n",
        "        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # seq_len * batch_size * vocab_size\n",
        "        return out\n",
        "\n",
        "    def step(self, x, h, c):\n",
        "        \"\"\"\n",
        "        Embeds input and applies LSTM one token at a time (seq_len = 1).\n",
        "        Inputs: x, h, c\n",
        "            - x: (batch_size, 1), sequence of tokens generated by generator\n",
        "            - h: (1, batch_size, hidden_dim), lstm hidden state\n",
        "            - c: (1, batch_size, hidden_dim), lstm cell state\n",
        "        Outputs: out, h, c\n",
        "            - out: (batch_size, 1, vocab_size), lstm output prediction\n",
        "            - h: (1, batch_size, hidden_dim), lstm hidden state\n",
        "            - c: (1, batch_size, hidden_dim), lstm cell state \n",
        "        \"\"\"\n",
        "        self.lstm.flatten_parameters()\n",
        "        emb = self.embed(x) # batch_size * 1 * emb_dim\n",
        "        out, (h, c) = self.lstm(emb, (h, c)) # out: batch_size * 1 * hidden_dim\n",
        "        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # batch_size * vocab_size\n",
        "        return out, h, c\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h = torch.zeros((1, batch_size, self.hidden_dim))\n",
        "        c = torch.zeros((1, batch_size, self.hidden_dim))\n",
        "        if self.use_cuda:\n",
        "            h, c = h.cuda(), c.cuda()\n",
        "        return h, c\n",
        "    \n",
        "    def init_params(self):\n",
        "        for param in self.parameters():\n",
        "            param.data.normal_(0, 1)\n",
        "\n",
        "    def sample(self, batch_size, seq_len):\n",
        "        \"\"\"\n",
        "        Samples the network and returns a batch of samples of length seq_len.\n",
        "        Outputs: out\n",
        "            - out: (batch_size * seq_len)\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        h, c = self.init_hidden(batch_size)\n",
        "        x = torch.zeros(batch_size, 1, dtype=torch.int64)\n",
        "        if self.use_cuda:\n",
        "            x = x.cuda()\n",
        "        for _ in range(seq_len):\n",
        "            out, h, c = self.step(x, h, c)\n",
        "            prob = torch.exp(out)\n",
        "            x = torch.multinomial(prob, 1)\n",
        "            samples.append(x)\n",
        "        out = torch.cat(samples, dim=1) # along the batch_size dimension\n",
        "        return out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB2j6Q8BRuje"
      },
      "source": [
        "## 4. PGLoss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF5SjZL3RwBu"
      },
      "source": [
        "class PGLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Pseudo-loss that gives corresponding policy gradients (on calling .backward()) \n",
        "    for adversial training of Generator\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PGLoss, self).__init__()\n",
        "\n",
        "    def forward(self, pred, target, reward):\n",
        "        \"\"\"\n",
        "        Inputs: pred, target, reward\n",
        "            - pred: (batch_size, seq_len), \n",
        "            - target : (batch_size, seq_len), \n",
        "            - reward : (batch_size, ), reward of each whole sentence\n",
        "        \"\"\"\n",
        "        one_hot = torch.zeros(pred.size(), dtype=torch.uint8)\n",
        "        if pred.is_cuda:\n",
        "            one_hot = one_hot.cuda()\n",
        "        one_hot.scatter_(1, target.data.view(-1, 1), 1)\n",
        "        loss = torch.masked_select(pred, one_hot)\n",
        "        loss = loss * reward.contiguous().view(-1)\n",
        "        loss = -torch.sum(loss)\n",
        "        return loss"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fus3bO-SP1V"
      },
      "source": [
        "##5. Data Iter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIQCUQyfSRG6"
      },
      "source": [
        "class GenDataIter:\n",
        "    \"\"\" Toy data iter to load digits \"\"\"\n",
        "\n",
        "    def __init__(self, data_file, batch_size):\n",
        "        super(GenDataIter, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.data_lis = self.read_file(data_file)\n",
        "        self.data_num = len(self.data_lis)\n",
        "        self.indices = range(self.data_num)\n",
        "        self.num_batches = math.ceil(self.data_num / self.batch_size)\n",
        "        self.idx = 0\n",
        "        self.reset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        return self.next()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.idx = 0\n",
        "        random.shuffle(self.data_lis)\n",
        "\n",
        "    def next(self):\n",
        "        if self.idx >= self.data_num:\n",
        "            raise StopIteration\n",
        "        index = self.indices[self.idx : self.idx + self.batch_size]\n",
        "        d = [self.data_lis[i] for i in index]\n",
        "        d = torch.tensor(d)\n",
        "\n",
        "        # 0 is prepended to d as start symbol\n",
        "        data = torch.cat([torch.zeros(len(index), 1, dtype=torch.int64), d], dim=1)\n",
        "        target = torch.cat([d, torch.zeros(len(index), 1, dtype=torch.int64)], dim=1)\n",
        "        \n",
        "        self.idx += self.batch_size\n",
        "        return data, target\n",
        "\n",
        "    def read_file(self, data_file):\n",
        "        with open(data_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        lis = []\n",
        "        for line in lines:\n",
        "            l = [int(s) for s in list(line.strip().split())]\n",
        "            lis.append(l)\n",
        "        return lis\n",
        "\n",
        "\n",
        "class DisDataIter:\n",
        "    \"\"\" Toy data iter to load digits \"\"\"\n",
        "\n",
        "    def __init__(self, real_data_file, fake_data_file, batch_size):\n",
        "        super(DisDataIter, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        real_data_lis = self.read_file(real_data_file)\n",
        "        fake_data_lis = self.read_file(fake_data_file)\n",
        "        self.data = real_data_lis + fake_data_lis\n",
        "        self.labels = [1 for _ in range(len(real_data_lis))] +\\\n",
        "                        [0 for _ in range(len(fake_data_lis))]\n",
        "        self.pairs = list(zip(self.data, self.labels))\n",
        "        self.data_num = len(self.pairs)\n",
        "        self.indices = range(self.data_num)\n",
        "        self.num_batches = math.ceil(self.data_num / self.batch_size)\n",
        "        self.idx = 0\n",
        "        self.reset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        return self.next()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.idx = 0\n",
        "        random.shuffle(self.pairs)\n",
        "\n",
        "    def next(self):\n",
        "        if self.idx >= self.data_num:\n",
        "            raise StopIteration\n",
        "        index = self.indices[self.idx : self.idx + self.batch_size]\n",
        "        pairs = [self.pairs[i] for i in index]\n",
        "        data = [p[0] for p in pairs]\n",
        "        label = [p[1] for p in pairs]\n",
        "        data = torch.tensor(data)\n",
        "        label = torch.tensor(label)\n",
        "        self.idx += self.batch_size\n",
        "        return data, label\n",
        "\n",
        "    def read_file(self, data_file):\n",
        "        with open(data_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        lis = []\n",
        "        for line in lines:\n",
        "            l = [int(s) for s in list(line.strip().split())]\n",
        "            lis.append(l) \n",
        "        return lis"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgbqe0LKVcLC"
      },
      "source": [
        "## 5. Rollout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPazU1vmVdHU"
      },
      "source": [
        "class Rollout(object):\n",
        "    \"\"\" Rollout Policy \"\"\"\n",
        "\n",
        "    def __init__(self, model, update_rate):\n",
        "        self.ori_model = model\n",
        "        self.own_model = copy.deepcopy(model)\n",
        "        self.update_rate = update_rate\n",
        "\n",
        "    def get_reward(self, x, num, discriminator):\n",
        "        \"\"\"\n",
        "        Inputs: x, num, discriminator\n",
        "            - x: (batch_size, seq_len) input data\n",
        "            - num: rollout number\n",
        "            - discriminator: discrimanator model\n",
        "        \"\"\"\n",
        "        rewards = []\n",
        "        batch_size = x.size(0)\n",
        "        seq_len = x.size(1)\n",
        "        for i in range(num):\n",
        "            for l in range(1, seq_len):\n",
        "                data = x[:, 0:l]\n",
        "                samples = self.own_model.sample(batch_size, seq_len, data)\n",
        "                pred = discriminator(samples)\n",
        "                pred = pred.cpu().data[:,1].numpy()\n",
        "                if i == 0:\n",
        "                    rewards.append(pred)\n",
        "                else:\n",
        "                    rewards[l-1] += pred\n",
        "\n",
        "            # for the last token\n",
        "            pred = discriminator(x)\n",
        "            pred = pred.cpu().data[:, 1].numpy()\n",
        "            if i == 0:\n",
        "                rewards.append(pred)\n",
        "            else:\n",
        "                rewards[seq_len-1] += pred\n",
        "        rewards = np.transpose(np.array(rewards)) / (1.0 * num) # batch_size * seq_len\n",
        "        return rewards\n",
        "\n",
        "    def update_params(self):\n",
        "        dic = {}\n",
        "        for name, param in self.ori_model.named_parameters():\n",
        "            dic[name] = param.data\n",
        "        for name, param in self.own_model.named_parameters():\n",
        "            if name.startswith('emb'):\n",
        "                param.data = dic[name]\n",
        "            else:\n",
        "                param.data = self.update_rate * param.data + (1 - self.update_rate) * dic[name]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DXaHb2CchoW"
      },
      "source": [
        "## 6. seqGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2-jkRIx2_bu"
      },
      "source": [
        "#########################################################################################\n",
        "#  Generator  Hyper-parameters\n",
        "######################################################################################\n",
        "g_embed_dim = 200 # embedding dimension (pretrained: 200, pk: 30)\n",
        "g_hidden_dim = 300 # hidden state dimension of lstm cell\n",
        "g_seq_len = 20 # sequence length\n",
        "START_TOKEN = 0\n",
        "\n",
        "#########################################################################################\n",
        "#  Discriminator  Hyper-parameters\n",
        "#########################################################################################\n",
        "d_num_class = 2\n",
        "d_embed_dim = 64\n",
        "d_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
        "d_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
        "d_dropout_prob = 0.75\n",
        "\n",
        "\n",
        "POSITIVE_FILE = 'real_data.txt'\n",
        "NEGATIVE_FILE = 'gene_data.txt'\n",
        "\n",
        "def generate_samples(model, batch_size, generated_num, output_file):\n",
        "    samples = []\n",
        "    for _ in range(int(generated_num / batch_size)):\n",
        "        sample = model.sample(batch_size, g_seq_len).cpu().data.numpy().tolist()\n",
        "        samples.extend(sample)\n",
        "    with open(output_file, 'w') as fout:\n",
        "        for sample in samples:\n",
        "            string = ' '.join([str(s) for s in sample])\n",
        "     \n",
        "            fout.write('{}\\n'.format(string))\n",
        "\n",
        "\n",
        "def train_generator_MLE(gen, data_iter, criterion, optimizer, epochs, \n",
        "        gen_pretrain_train_loss, args):\n",
        "    \"\"\"\n",
        "    Train generator with MLE\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.\n",
        "        for data, target in data_iter:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            target = target.contiguous().view(-1)\n",
        "            output = gen(data)\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        data_iter.reset()\n",
        "    avg_loss = total_loss / len(data_iter)\n",
        "    print(\"Epoch {}, train loss: {:.5f}\".format(epoch, avg_loss))\n",
        "    gen_pretrain_train_loss.append(avg_loss)\n",
        "\n",
        "def train_generator_PG(gen, dis, rollout, pg_loss, optimizer, epochs, args):\n",
        "    \"\"\"\n",
        "    Train generator with the guidance of policy gradient\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        # construct the input to the genrator, add zeros before samples and delete the last column\n",
        "        samples = generator.sample(args.batch_size, g_seq_len)\n",
        "        zeros = torch.zeros(args.batch_size, 1, dtype=torch.int64)\n",
        "        if samples.is_cuda:\n",
        "            zeros = zeros.cuda()\n",
        "        inputs = torch.cat([zeros, samples.data], dim = 1)[:, :-1].contiguous()\n",
        "        targets = samples.data.contiguous().view((-1,))\n",
        "\n",
        "        # calculate the reward\n",
        "        rewards = torch.tensor(rollout.get_reward(samples, args.n_rollout, dis))\n",
        "        if args.cuda:\n",
        "            rewards = rewards.cuda()\n",
        "\n",
        "        # update generator\n",
        "        output = gen(inputs)\n",
        "        loss = pg_loss(output, targets, rewards)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def eval_generator(model, data_iter, criterion, args):\n",
        "    \"\"\"\n",
        "    Evaluate generator with NLL\n",
        "    \"\"\"\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_iter:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            target = target.contiguous().view(-1)\n",
        "            pred = model(data)\n",
        "            loss = criterion(pred, target)\n",
        "            total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(data_iter)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def train_discriminator(dis, gen, criterion, optimizer, epochs, \n",
        "        dis_adversarial_train_loss, dis_adversarial_train_acc, args):\n",
        "    \"\"\"\n",
        "    Train discriminator\n",
        "    \"\"\"\n",
        "    generate_samples(gen, args.batch_size, args.n_samples, NEGATIVE_FILE)\n",
        "    data_iter = DisDataIter(POSITIVE_FILE, NEGATIVE_FILE, args.batch_size)\n",
        "    for epoch in range(epochs):\n",
        "        correct = 0\n",
        "        total_loss = 0.\n",
        "        for data, target in data_iter:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            target = target.contiguous().view(-1)\n",
        "            output = dis(data)\n",
        "            pred = output.data.max(1)[1]\n",
        "            correct += pred.eq(target.data).cpu().sum()\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        data_iter.reset()\n",
        "        avg_loss = total_loss / len(data_iter)\n",
        "        acc = correct.item() / data_iter.data_num\n",
        "        print(\"Epoch {}, train loss: {:.5f}, train acc: {:.3f}\".format(epoch, avg_loss, acc))\n",
        "        dis_adversarial_train_loss.append(avg_loss)\n",
        "        dis_adversarial_train_acc.append(acc)\n",
        "\n",
        "\n",
        "def eval_discriminator(model, data_iter, criterion, args):\n",
        "    \"\"\"\n",
        "    Evaluate discriminator, dropout is enabled\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_iter:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            target = target.contiguous().view(-1)\n",
        "            output = model(data)\n",
        "            pred = output.data.max(1)[1]\n",
        "            correct += pred.eq(target.data).cpu().sum()\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(data_iter)\n",
        "    acc = correct.item() / data_iter.data_num\n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "def adversarial_train(gen, dis, rollout, pg_loss, nll_loss, gen_optimizer, dis_optimizer, \n",
        "        dis_adversarial_train_loss, dis_adversarial_train_acc, args):\n",
        "    \"\"\"\n",
        "    Adversarially train generator and discriminator\n",
        "    \"\"\"\n",
        "    # train generator for g_steps\n",
        "    print(\"#Train generator\")\n",
        "    for i in range(args.g_steps):\n",
        "        print(\"##G-Step {}\".format(i))\n",
        "        train_generator_PG(gen, dis, rollout, pg_loss, gen_optimizer, args.gk_epochs, args)\n",
        "\n",
        "    # train discriminator for d_steps\n",
        "    print(\"#Train discriminator\")\n",
        "    for i in range(args.d_steps):\n",
        "        print(\"##D-Step {}\".format(i))\n",
        "        train_discriminator(dis, gen, nll_loss, dis_optimizer, args.dk_epochs, \n",
        "            dis_adversarial_train_loss, dis_adversarial_train_acc, args)\n",
        "\n",
        "    # update roll-out model\n",
        "    rollout.update_params()\n",
        "\n",
        "def translating_sample(sample_path , voca_path):\n",
        "\n",
        "  print(\"================================================\")\n",
        "  print(\"생성된 데이터 확인.....\")\n",
        "  df = pd.read_csv(sample_path , names = [\"data\"] , sep = \"\\t\" )\n",
        "  Sentences = [sentence.split(\" \") for sentence in df['data'].values]\n",
        "  print(\"생성된 가사 수 : {} , 가사 시퀀스 수 : {} \".format(len(Sentences) , len(Sentences[0])))\n",
        "  print(\" \")\n",
        "\n",
        "  print(\"저장된 단어사전 확인....\")\n",
        "  a = open(voca_path, 'rb')\n",
        "  voca = pickle.load(a)\n",
        "  print(\"단어 사전에 등록된 단어 수 : \" , len(voca))\n",
        "  print(\" \")\n",
        "\n",
        "  # 확인할 무작위 시드 / 사이즈\n",
        "  random_seed = 100\n",
        "  random_size = 10\n",
        "\n",
        "  test_Sentences = Sentences[random_seed:random_seed + random_size]\n",
        "\n",
        "\n",
        "  for sentence in test_Sentences:\n",
        "    real_sentence = \"\"\n",
        "    for s in sentence:\n",
        "\n",
        "      try:\n",
        "        s = int(s)\n",
        "      except: pass\n",
        "\n",
        "      if voca.get(s):\n",
        "        if voca[s] == \"UNK\":\n",
        "          break\n",
        "        real_sentence += voca[s] + \" \"\n",
        "\n",
        "    print(real_sentence)\n",
        "    print(\"------------------------\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4pBdHhdYip6"
      },
      "source": [
        "## SeqGAN 실행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KUxu4VkYpst"
      },
      "source": [
        "### 세팅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT4EqrY_rQtu",
        "outputId": "9e2fcb03-80a9-4b4f-a323-2e4508c034a4"
      },
      "source": [
        "# Arguemnts\n",
        "parser = argparse.ArgumentParser(description='SeqGAN')\n",
        "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
        "parser.add_argument('--hpc', action='store_true', default=False,\n",
        "                    help='set to hpc mode')\n",
        "parser.add_argument('--data_path', type=str, default='/content/data/', metavar='PATH',\n",
        "                    help='data path to save files (default: /content/data/)')\n",
        "parser.add_argument('--rounds', type=int, default=150, metavar='N',\n",
        "                    help='rounds of adversarial training (default: 150)')\n",
        "parser.add_argument('--g_pretrain_steps', type=int, default=10, metavar='N',\n",
        "                    help='steps of pre-training of generators (default: 120)')\n",
        "parser.add_argument('--d_pretrain_steps', type=int, default=50, metavar='N',\n",
        "                    help='steps of pre-training of discriminators (default: 50)')\n",
        "parser.add_argument('--g_steps', type=int, default=1, metavar='N',\n",
        "                    help='steps of generator updates in one round of adverarial training (default: 1)')\n",
        "parser.add_argument('--d_steps', type=int, default=3, metavar='N',\n",
        "                    help='steps of discriminator updates in one round of adverarial training (default: 3)')\n",
        "parser.add_argument('--gk_epochs', type=int, default=1, metavar='N',\n",
        "                    help='epochs of generator updates in one step of generate update (default: 1)')\n",
        "parser.add_argument('--dk_epochs', type=int, default=3, metavar='N',\n",
        "                    help='epochs of discriminator updates in one step of discriminator update (default: 3)')\n",
        "parser.add_argument('--update_rate', type=float, default=0.8, metavar='UR',\n",
        "                    help='update rate of roll-out model (default: 0.8)')\n",
        "parser.add_argument('--n_rollout', type=int, default=16, metavar='N',\n",
        "                    help='number of roll-out (default: 16)')\n",
        "parser.add_argument('--vocab_size', type=int, default=10365, metavar='N',\n",
        "                    help='vocabulary size (default: 11292)')\n",
        "parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
        "                    help='batch size (default: 64)')\n",
        "parser.add_argument('--n_samples', type=int, default=6400, metavar='N',\n",
        "                    help='number of samples gerenated per time (default: 6400)')\n",
        "parser.add_argument('--gen_lr', type=float, default=1e-3, metavar='LR',\n",
        "                    help='learning rate of generator optimizer (default: 1e-3)')\n",
        "parser.add_argument('--dis_lr', type=float, default=1e-3, metavar='LR',\n",
        "                    help='learning rate of discriminator optimizer (default: 1e-3)')\n",
        "parser.add_argument('--no_cuda', action='store_true', default=False,\n",
        "                    help='disables CUDA training')\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                    help='random seed (default: 1)')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--seed'], dest='seed', nargs=None, const=None, default=1, type=<class 'int'>, choices=None, help='random seed (default: 1)', metavar='S')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP_yCBl9YlkX"
      },
      "source": [
        "# Parse arguments\n",
        "args = parser.parse_args()\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "if not args.hpc:\n",
        "    args.data_path = ''\n",
        "\n",
        "# Positive_File / Negative_File\n",
        "POSITIVE_FILE = args.data_path + POSITIVE_FILE\n",
        "NEGATIVE_FILE = args.data_path + NEGATIVE_FILE"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfr7klavYv2X"
      },
      "source": [
        "### 모델 로스 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnvjujWZYu5H"
      },
      "source": [
        "# Set models, criteria, optimizers\n",
        "generator = Generator(args.vocab_size, g_embed_dim, g_hidden_dim, args.cuda)\n",
        "discriminator = Discriminator(d_num_class, args.vocab_size, d_embed_dim, d_filter_sizes, d_num_filters, d_dropout_prob)\n",
        "# target_lstm = TargetLSTM(args.vocab_size, g_embed_dim, g_hidden_dim, args.cuda)\n",
        "\n",
        "nll_loss = nn.NLLLoss()\n",
        "pg_loss = PGLoss()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trOw8CSjY5b6"
      },
      "source": [
        "### cuda/optimizer 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm71wnz9Y5EZ"
      },
      "source": [
        "if args.cuda:\n",
        "    generator = generator.cuda()\n",
        "    discriminator = discriminator.cuda()\n",
        "    # target_lstm = target_lstm.cuda()\n",
        "    nll_loss = nll_loss.cuda()\n",
        "    pg_loss = pg_loss.cuda()\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "gen_optimizer = optim.Adam(params=generator.parameters(), lr=args.gen_lr)\n",
        "dis_optimizer = optim.SGD(params=discriminator.parameters(), lr=args.dis_lr)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1djRuZW5ZB9r"
      },
      "source": [
        "### Pretrain Generator / Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIY3AtPuZA3e"
      },
      "source": [
        "# Container of experiment data\n",
        "gen_pretrain_train_loss = []\n",
        "gen_pretrain_eval_loss = []\n",
        "dis_pretrain_train_loss = []\n",
        "dis_pretrain_train_acc = []\n",
        "dis_pretrain_eval_loss = []\n",
        "dis_pretrain_eval_acc = []\n",
        "gen_adversarial_eval_loss = []\n",
        "dis_adversarial_train_loss = []\n",
        "dis_adversarial_train_acc = []\n",
        "dis_adversarial_eval_loss = []\n",
        "dis_adversarial_eval_acc = []"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynz5thM-SEw6",
        "outputId": "f6561db8-7e57-4683-d41b-6927e6db9952"
      },
      "source": [
        "# # Generate toy data using target LSTM\n",
        "# print('#####################################################')\n",
        "# print('Generating data ...')\n",
        "# print('#####################################################\\n\\n')\n",
        "# generate_samples(target_lstm, args.batch_size, args.n_samples, POSITIVE_FILE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#####################################################\n",
            "Generating data ...\n",
            "#####################################################\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f7BYA8hZI_f",
        "outputId": "9b0199d1-e6cd-45f0-f100-69c0a7a2a814"
      },
      "source": [
        "MODEL_PATH = \"/content/generator.pt\"\n",
        "VOCA_PATH = \"/content/data/idx2pos.pkl\"\n",
        "GENERATE_PATH = \"/content/gene_data.txt\"\n",
        "\n",
        "# Pre-train generator using MLE\n",
        "print('#####################################################')\n",
        "print('Start pre-training generator with MLE...')\n",
        "print('#####################################################\\n')\n",
        "gen_data_iter = GenDataIter(POSITIVE_FILE, args.batch_size)\n",
        "\n",
        "#for i in range(args.g_pretrain_steps):\n",
        "for i in range(1):      \n",
        "    print(\"G-Step {}\".format(i))\n",
        "    train_generator_MLE(generator, gen_data_iter, nll_loss, \n",
        "        gen_optimizer, args.gk_epochs, \n",
        "        gen_pretrain_train_loss, args)\n",
        "    generate_samples(generator, args.batch_size, args.n_samples, NEGATIVE_FILE)\n",
        "    eval_iter = GenDataIter(NEGATIVE_FILE, args.batch_size)\n",
        "    gen_loss = eval_generator(generator, eval_iter, nll_loss, args)\n",
        "    gen_pretrain_eval_loss.append(gen_loss)\n",
        "    print(\"eval loss: {:.5f}\\n\".format(gen_loss))\n",
        "    translating_sample(GENERATE_PATH , VOCA_PATH)\n",
        "\n",
        "print('#####################################################\\n\\n')\n",
        "\n",
        "print(\"Pretrain Generator Model Save...!\")\n",
        "torch.save(generator , MODEL_PATH)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#####################################################\n",
            "Start pre-training generator with MLE...\n",
            "#####################################################\n",
            "\n",
            "G-Step 0\n",
            "Epoch 0, train loss: 3.81953\n",
            "eval loss: 3.48328\n",
            "\n",
            "================================================\n",
            "생성된 데이터 확인.....\n",
            "생성된 가사 수 : 6400 , 가사 시퀀스 수 : 20 \n",
            " \n",
            "저장된 단어사전 확인....\n",
            "단어 사전에 등록된 단어 수 :  10365\n",
            " \n",
            "아플 삶 푸른 에게 머릿결 열정 순간 같아요 별 \n",
            "------------------------\n",
            "\n",
            "------------------------\n",
            "살아요 내 려고 있었는데 저 는 넘친 가나 좋아 요 가는 슬픔 \n",
            "------------------------\n",
            "너무 바라만 보다가도 너 하지 와 봐 말 \n",
            "------------------------\n",
            "너무 화려한 너 의 진정 \n",
            "------------------------\n",
            "네 위 모두 오는 \n",
            "------------------------\n",
            "도무지 쓰면 소리 \n",
            "------------------------\n",
            "헛된 이 오늘 길 \n",
            "------------------------\n",
            "다시는 에 처음 긴 좋았었는지 모든 만 봐요 에 끝 건 \n",
            "------------------------\n",
            "그대 같이 나일 맘 을 멸망하네 이 땐 에 처럼 \n",
            "------------------------\n",
            "#####################################################\n",
            "\n",
            "\n",
            "Pretrain Generator Model Save...!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_blBPplpj6vm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f533acc-2e48-4033-d4e7-4d925c8de40b"
      },
      "source": [
        "VOCA_PATH = \"/content/data/idx2pos.pkl\"\n",
        "GENERATE_PATH = \"/content/gene_data.txt\"\n",
        "\n",
        "translating_sample(GENERATE_PATH , VOCA_PATH)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================\n",
            "생성된 데이터 확인.....\n",
            "생성된 가사 수 : 6400 , 가사 시퀀스 수 : 20 \n",
            " \n",
            "저장된 단어사전 확인....\n",
            "단어 사전에 등록된 단어 수 :  10365\n",
            " \n",
            "아플 삶 푸른 고민 머릿결 열정 알 같아요 떠오르는 마음 \n",
            "------------------------\n",
            "\n",
            "------------------------\n",
            "살아요 내 려고 있었는데 저 는 넘친 가나 좋아 요 가는 슬픔 \n",
            "------------------------\n",
            "놀라워라 바라만 약속 너 하지 와 봐 말 \n",
            "------------------------\n",
            "너무 화려한 너 에 진정 \n",
            "------------------------\n",
            "달라질 위 \n",
            "------------------------\n",
            "난 쓰면 소리 여기 터질듯 같은 의 딱 혹시 낼거야 는 그럼 에 하지도 할 할 지금 이 봐 <start> \n",
            "------------------------\n",
            "헛된 이 오늘 길 \n",
            "------------------------\n",
            "다시는 끝나지 처음 미안해 좋았었는지 모든 만 봐요 에 쌓이던 건 \n",
            "------------------------\n",
            "그대 같이 나일 맘 을 멸망하네 이 건지 에 처럼 \n",
            "------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRyuHVLrUmJX",
        "outputId": "c6196881-579e-467e-f418-29fd3f2d72a0"
      },
      "source": [
        "# Pre-train discriminator\n",
        "print('#####################################################')\n",
        "print('Start pre-training discriminator...')\n",
        "print('#####################################################\\n')\n",
        "for i in range(5):\n",
        "#for i in range(args.d_pretrain_steps):      \n",
        "    print(\"D-Step {}\".format(i))\n",
        "    train_discriminator(discriminator, generator, nll_loss, \n",
        "        dis_optimizer, args.dk_epochs, \n",
        "        dis_adversarial_train_loss, dis_adversarial_train_acc, args)\n",
        "    generate_samples(generator, args.batch_size, args.n_samples, NEGATIVE_FILE)\n",
        "    eval_iter = DisDataIter(POSITIVE_FILE, NEGATIVE_FILE, args.batch_size)\n",
        "    dis_loss, dis_acc = eval_discriminator(discriminator, eval_iter, nll_loss, args)\n",
        "    dis_pretrain_eval_loss.append(dis_loss)\n",
        "    dis_pretrain_eval_acc.append(dis_acc)\n",
        "    print(\"eval loss: {:.5f}, eval acc: {:.3f}\\n\".format(dis_loss, dis_acc))\n",
        "print('#####################################################\\n\\n')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#####################################################\n",
            "Start pre-training discriminator...\n",
            "#####################################################\n",
            "\n",
            "D-Step 0\n",
            "Epoch 0, train loss: 0.58275, train acc: 0.747\n",
            "Epoch 1, train loss: 0.54524, train acc: 0.763\n",
            "Epoch 2, train loss: 0.51284, train acc: 0.778\n",
            "eval loss: 0.50015, eval acc: 0.788\n",
            "\n",
            "D-Step 1\n",
            "Epoch 0, train loss: 0.48372, train acc: 0.797\n",
            "Epoch 1, train loss: 0.45440, train acc: 0.822\n",
            "Epoch 2, train loss: 0.43637, train acc: 0.838\n",
            "eval loss: 0.42974, eval acc: 0.844\n",
            "\n",
            "D-Step 2\n",
            "Epoch 0, train loss: 0.42190, train acc: 0.848\n",
            "Epoch 1, train loss: 0.40771, train acc: 0.858\n",
            "Epoch 2, train loss: 0.39750, train acc: 0.865\n",
            "eval loss: 0.39436, eval acc: 0.867\n",
            "\n",
            "D-Step 3\n",
            "Epoch 0, train loss: 0.38625, train acc: 0.871\n",
            "Epoch 1, train loss: 0.38282, train acc: 0.874\n",
            "Epoch 2, train loss: 0.37228, train acc: 0.877\n",
            "eval loss: 0.37089, eval acc: 0.879\n",
            "\n",
            "D-Step 4\n",
            "Epoch 0, train loss: 0.37488, train acc: 0.876\n",
            "Epoch 1, train loss: 0.37077, train acc: 0.877\n",
            "Epoch 2, train loss: 0.36590, train acc: 0.882\n",
            "eval loss: 0.35764, eval acc: 0.883\n",
            "\n",
            "#####################################################\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPPoUiaeZsB5",
        "outputId": "85c53bfa-719e-4b8f-8fe1-e3bc62b5bfdd"
      },
      "source": [
        "generator = torch.load(MODEL_PATH)\n",
        "\n",
        "generate_samples(generator, args.batch_size, args.n_samples, NEGATIVE_FILE)\n",
        "eval_iter = GenDataIter(NEGATIVE_FILE, args.batch_size)\n",
        "gen_loss = eval_generator(generator, eval_iter, nll_loss, args)\n",
        "gen_pretrain_eval_loss.append(gen_loss)\n",
        "print(\"eval loss: {:.5f}\\n\".format(gen_loss))\n",
        "\n",
        "translating_sample(GENERATE_PATH , VOCA_PATH)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eval loss: 3.51196\n",
            "\n",
            "================================================\n",
            "생성된 데이터 확인.....\n",
            "생성된 가사 수 : 6400 , 가사 시퀀스 수 : 20 \n",
            " \n",
            "저장된 단어사전 확인....\n",
            "단어 사전에 등록된 단어 수 :  10365\n",
            " \n",
            "데리러 펼치고 내 뿐 한 매력 저 마음 소리쳐 랑 \n",
            "------------------------\n",
            "또 발그레 생각 들 에서 종일 평생 한 눈물 줘요 \n",
            "------------------------\n",
            "날 맞대 우우 이 영원한 자신 에서 \n",
            "------------------------\n",
            "이 타 세탁소 마음 하고 처럼 해주지 만난 때 \n",
            "------------------------\n",
            "농담 그대 이 묻어주던 나을 모든 바람 넌 한 기억 \n",
            "------------------------\n",
            "오직 우 의 해 나를 태웠는지 \n",
            "------------------------\n",
            "길가 너 그 걸 을 그냥 쉴 모습 처럼 \n",
            "------------------------\n",
            "하루 정말 가 그만 더 나눈 우주 이 움큼 난 않게 \n",
            "------------------------\n",
            "잊고 언젠가 그게 을 이내 잖아 내 들 가 내 나이 말 돌려주고 채워 세상 을 달라진 전 대도 \n",
            "------------------------\n",
            "그렇게 혼자 향 오늘 의 처음 이 먼저 무너지는 안되나 너 없을테니 \n",
            "------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJvFmKXPZyi_",
        "outputId": "67e0dc75-cd0f-4b0b-8ef2-80b69eecf22e"
      },
      "source": [
        "translating_sample(GENERATE_PATH , VOCA_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================\n",
            "생성된 데이터 확인.....\n",
            "생성된 가사 수 : 6400 , 가사 시퀀스 수 : 20 \n",
            " \n",
            "저장된 단어사전 확인....\n",
            "단어 사전에 등록된 단어 수 :  10365\n",
            " \n",
            "울 어도 울 어도 네 가 돌아올 수 없다면 \n",
            "------------------------\n",
            "내 맘 에 네 온 날 느린 것 같아 외로움 \n",
            "------------------------\n",
            "누구 라도 모를거에요 창문 을 열어요 새 들 을 불러요 \n",
            "------------------------\n",
            "\n",
            "------------------------\n",
            "행복해지는 동화 를 듣고 자라나지 아름답고 착한 사람 들 은 \n",
            "------------------------\n",
            "잎새 에 이는 바람 에도 난 괴롭다 했잖아 \n",
            "------------------------\n",
            "그대 왜 자꾸 웃어 줬나요 아무 상관없는 그대 삶 의 나라 면 \n",
            "------------------------\n",
            "당신 어깨 를 둘 수 있게 더 이상 아무 말 도 할 수가 없지만 \n",
            "------------------------\n",
            "아득히 멀기만 한 내 미래 를 비춰줄 끝없이 이끌어 줄 에 아름다운 마음 난 난 \n",
            "------------------------\n",
            "어쩌면 조금 우리 의 몸 을 지켜봐 주는 것 같아요 가슴 으로 느껴 보세요 \n",
            "------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXwecpmgaBbV"
      },
      "source": [
        "### Adversarial Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "578CQJE4ZlIG",
        "outputId": "cb74811d-a12e-48f9-d107-e1ca7fc492ad"
      },
      "source": [
        "# Adversarial training\n",
        "print('#####################################################')\n",
        "print('Start adversarial training...')\n",
        "print('#####################################################\\n')\n",
        "rollout = Rollout(generator, args.update_rate)\n",
        "#for i in range(1):    \n",
        "for i in range(args.rounds):\n",
        "    print(\"Round {}\".format(i))\n",
        "    adversarial_train(generator, discriminator, rollout, \n",
        "        pg_loss, nll_loss, gen_optimizer, dis_optimizer, \n",
        "        dis_adversarial_train_loss, dis_adversarial_train_acc, args)\n",
        "    generate_samples(generator, args.batch_size, args.n_samples, NEGATIVE_FILE)\n",
        "    gen_eval_iter = GenDataIter(NEGATIVE_FILE, args.batch_size)\n",
        "    dis_eval_iter = DisDataIter(POSITIVE_FILE, NEGATIVE_FILE, args.batch_size)\n",
        "    gen_loss = eval_generator(generator, gen_eval_iter, nll_loss, args)\n",
        "    gen_adversarial_eval_loss.append(gen_loss)\n",
        "    dis_loss, dis_acc = eval_discriminator(discriminator, dis_eval_iter, nll_loss, args)\n",
        "    dis_adversarial_eval_loss.append(dis_loss)\n",
        "    dis_adversarial_eval_acc.append(dis_acc)\n",
        "    translating_sample(GENERATE_PATH , VOCA_PATH)\n",
        "    print(\"gen eval loss: {:.5f}, dis eval loss: {:.5f}, dis eval acc: {:.3f}\\n\"\n",
        "        .format(gen_loss, dis_loss, dis_acc))\n",
        "\n",
        "# Save experiment data\n",
        "with open(args.data_path + 'experiment.pkl', 'wb') as f:\n",
        "    pickle.dump(\n",
        "        (gen_pretrain_train_loss,\n",
        "            gen_pretrain_eval_loss,\n",
        "            dis_pretrain_train_loss,\n",
        "            dis_pretrain_train_acc,\n",
        "            dis_pretrain_eval_loss,\n",
        "            dis_pretrain_eval_acc,\n",
        "            gen_adversarial_eval_loss,\n",
        "            dis_adversarial_train_loss,\n",
        "            dis_adversarial_train_acc,\n",
        "            dis_adversarial_eval_loss,\n",
        "            dis_adversarial_eval_acc),\n",
        "        f,\n",
        "        protocol=pickle.HIGHEST_PROTOCOL\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#####################################################\n",
            "Start adversarial training...\n",
            "#####################################################\n",
            "\n",
            "Round 0\n",
            "#Train generator\n",
            "##G-Step 0\n",
            "#Train discriminator\n",
            "##D-Step 0\n",
            "Epoch 0, train loss: 0.26438, train acc: 0.908\n",
            "Epoch 1, train loss: 0.25544, train acc: 0.912\n",
            "Epoch 2, train loss: 0.24831, train acc: 0.917\n",
            "##D-Step 1\n",
            "Epoch 0, train loss: 0.24401, train acc: 0.919\n",
            "Epoch 1, train loss: 0.24217, train acc: 0.921\n",
            "Epoch 2, train loss: 0.24279, train acc: 0.921\n",
            "##D-Step 2\n",
            "Epoch 0, train loss: 0.24126, train acc: 0.922\n",
            "Epoch 1, train loss: 0.24159, train acc: 0.922\n",
            "Epoch 2, train loss: 0.23836, train acc: 0.924\n",
            "================================================\n",
            "생성된 데이터 확인.....\n",
            "생성된 가사 수 : 6400 , 가사 시퀀스 수 : 20 \n",
            " \n",
            "저장된 단어사전 확인....\n",
            "단어 사전에 등록된 단어 수 :  10365\n",
            " \n",
            "샤워 사는 로 일만 저 때 남자 만일 그냥 영원할 없나요 에 변함 고독 미친 가본 나를 하면서 우리 웃어 \n",
            "------------------------\n",
            "순식간 말 두 거리 이 퍼질 몰래 싶던 보내진 추억 마음 일 괜찮잖아 조그만 괴롭혔지 하는 그 흘려 침착하게 숨어가고 \n",
            "------------------------\n",
            "길 라라라라 기다리고 내 을 살아가지 오랜 가 를 버리겠어 충만 맡긴 때 이렇게 사랑 갤 있잖아 다 멀다하고 솔직하게 \n",
            "------------------------\n",
            "라고 이 갔고 손잡고 뿐 모두 이 떠난다 나를 들어온 한 뿐 나서 바란 있어요 이제 가릴 도 뭘 지나간 \n",
            "------------------------\n",
            "흩날리는 우 모습 연못 \n",
            "------------------------\n",
            "에 좀 다시 를 보기 어루 려 후회되는 의 斂 인연 우리 많은 한번 있었네 미안하단 삼으라 가 와인 차라리 \n",
            "------------------------\n",
            "그대 하기 무게 의 느껴 되돌려 날 도쿄 이 사랑 도 떠나 외로웠었던 한 하여 다른 롤 난 꽃들이 술 \n",
            "------------------------\n",
            "마법 만 없이 토록 긴 이 밝았는데 를 다른 진 아닌데 단 을 날 영원할 그 알 만이 들 또 \n",
            "------------------------\n",
            "내 미안해 널 사랑 했단 이 지 지나면 하고픈 를 아니라고는 나 애 책 그냥 가 무얼 감은 너무 예쁘지 \n",
            "------------------------\n",
            "기다린단 태어난 처음 늦었나 게 지금 는 달아요 해줬는데 있다 은 모두 우린 모습 제일 길 하자 내게 더위 이 \n",
            "------------------------\n",
            "gen eval loss: 6.99523, dis eval loss: 0.23455, dis eval acc: 0.925\n",
            "\n",
            "Round 1\n",
            "#Train generator\n",
            "##G-Step 0\n",
            "#Train discriminator\n",
            "##D-Step 0\n",
            "Epoch 0, train loss: 0.19262, train acc: 0.938\n",
            "Epoch 1, train loss: 0.19052, train acc: 0.939\n",
            "Epoch 2, train loss: 0.19033, train acc: 0.940\n",
            "##D-Step 1\n",
            "Epoch 0, train loss: 0.19050, train acc: 0.940\n",
            "Epoch 1, train loss: 0.18727, train acc: 0.941\n",
            "Epoch 2, train loss: 0.18878, train acc: 0.941\n",
            "##D-Step 2\n",
            "Epoch 0, train loss: 0.17941, train acc: 0.943\n",
            "Epoch 1, train loss: 0.17825, train acc: 0.943\n",
            "Epoch 2, train loss: 0.17778, train acc: 0.943\n",
            "================================================\n",
            "생성된 데이터 확인.....\n",
            "생성된 가사 수 : 6400 , 가사 시퀀스 수 : 20 \n",
            " \n",
            "저장된 단어사전 확인....\n",
            "단어 사전에 등록된 단어 수 :  10365\n",
            " \n",
            "여보세요 가 안되는 뜨겁게 그 에도 삶 보다 걸음 에게 에 생각 애가 우리 누구 네 이 이 늙지 안되는지 \n",
            "------------------------\n",
            "한 소모 손 축복 집착 마음 갖고 피 수 이어 정상 난 이 늦은 더디게 옛 곳 잘 생각 흔들릴까 \n",
            "------------------------\n",
            "또 야 꿈 이 있기를 꽃 언제 요 있도록 건지도 으로 쳐진 우리 것 가 든 큰 마시며 주세요 아픈 \n",
            "------------------------\n",
            "귀 녹두 무엇 만일 그대 그저 누굴 여보세요 는 흔들어 좋은 에다 우리 말 평생 마지막 들려오는 이렇게 봐 잠시 \n",
            "------------------------\n",
            "이 보는 많은 되게 이제 거고 점 일부러 지난 하지만 니 이제 의 니 에 떠난 싶을 끝 말 그 \n",
            "------------------------\n",
            "만들고 둘러댈 \n",
            "------------------------\n",
            "이제 을 을 두려워 짜 뎌 여전히 앞 나 하나 와 무엇 봐 이 밭 버린 만큼이나 와 로 강하지 \n",
            "------------------------\n",
            "하기를 날 걱정 를 빼면 소음 을 때마침 이 떠나가고 그렇게 돌리면 한 앉아 날 그렇게 아파 들고 보니 기다리니까 \n",
            "------------------------\n",
            "\n",
            "------------------------\n",
            "까지도 날 남아 하루 의 긴 다시 빨랐다면 않으니 것 를 떠나가고 일분 기뻐 젠 거리 라서 이렇다면 마음 못 \n",
            "------------------------\n",
            "gen eval loss: 7.23316, dis eval loss: 0.18688, dis eval acc: 0.941\n",
            "\n",
            "Round 2\n",
            "#Train generator\n",
            "##G-Step 0\n",
            "#Train discriminator\n",
            "##D-Step 0\n",
            "Epoch 0, train loss: 0.19239, train acc: 0.941\n",
            "Epoch 1, train loss: 0.18642, train acc: 0.942\n",
            "Epoch 2, train loss: 0.18882, train acc: 0.942\n",
            "##D-Step 1\n",
            "Epoch 0, train loss: 0.18953, train acc: 0.941\n",
            "Epoch 1, train loss: 0.18818, train acc: 0.942\n",
            "Epoch 2, train loss: 0.18731, train acc: 0.942\n",
            "##D-Step 2\n",
            "Epoch 0, train loss: 0.18631, train acc: 0.942\n",
            "Epoch 1, train loss: 0.18696, train acc: 0.941\n",
            "Epoch 2, train loss: 0.18580, train acc: 0.941\n",
            "================================================\n",
            "생성된 데이터 확인.....\n",
            "생성된 가사 수 : 6400 , 가사 시퀀스 수 : 20 \n",
            " \n",
            "저장된 단어사전 확인....\n",
            "단어 사전에 등록된 단어 수 :  10365\n",
            " \n",
            "꼭 만큼 꼬불꼬불 사랑 날 의 알잖아 그리움 도 외출 노래 사랑 거니 겨울 이었나 이제 그저 방금 힘써서 무엇 \n",
            "------------------------\n",
            "어 빠르게 고향 미운 금연 무 이 봐 우리 이 불어와 오던 울면 떠나가세요 그저 다 꿈이었잖아 뉴 못 이끌어 \n",
            "------------------------\n",
            "너 되줄께요 너 우리 는 내일 했기에 그리운 멋지잖아요 달콤한 시간 찬양 봄 가 이 재운다 내릴 옆 그토록 떠오르는데 \n",
            "------------------------\n",
            "언짢은 사진 나 처음 너 때 웃게 어떡하나요 이쁜 봐도 을 얘기 이 지나고 할까 이유 토록 사실 번호 그 \n",
            "------------------------\n",
            "음악 너 고요해진 않고 가파른 부르던 으론 밤 혼자 나겠죠 날아들고 나 을 굴어도 탐내지 하면 스친 사랑 부질없다 항상 \n",
            "------------------------\n",
            "길 기뻐 했었건 이었대 니 그토록 텔 너 있잖아 그 환한 생각나 인가 이 털어 그리운 그대 내 싶어라 \n",
            "------------------------\n",
            "다시 인 아직 사는지 따스한 들녁 처음 그때 부린 그때 태양 그리워진 는 사랑 들이 못 창문 밤 그래 그 \n",
            "------------------------\n",
            "주세요 지낸다고 아니지만 나 너 내 웃었죠 한 했어 그 찡그리네요 의 하얘지도록 아픈 그 신기하게 위로 아플 너 떠났어도 \n",
            "------------------------\n",
            "자 끊겠어 을 달콤했었고 앞 미룬 이 스럽게 \n",
            "------------------------\n",
            "둘러보니 미운 는 의 오져요 이 날 제자 바라죠 않은 사랑 말아요 그 맞는 않아 눈 손 사이사이 답답한 을 \n",
            "------------------------\n",
            "gen eval loss: 7.26794, dis eval loss: 0.18479, dis eval acc: 0.942\n",
            "\n",
            "Round 3\n",
            "#Train generator\n",
            "##G-Step 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNJ6yuDaysfq"
      },
      "source": [
        "# MODEL_PATH = \"/content/generator_1.pt\"\n",
        "\n",
        "torch.save(generator, \"/content/generator_2.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1QbpH2QzBas"
      },
      "source": [
        "generator = torch.load(\"/content/generator_1.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "U-3LHU6RxMIz",
        "outputId": "bc49db83-6900-41b0-f959-6a299d5ecfd9"
      },
      "source": [
        "translating_sample('/content/real_data.txt' , VOCA_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================\n",
            "생성된 데이터 확인.....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-83aaff1f0b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslating_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/real_data.txt'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mVOCA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-123-fc5cd9db386d>\u001b[0m in \u001b[0;36mtranslating_sample\u001b[0;34m(sample_path, voca_path)\u001b[0m\n\u001b[1;32m    226\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"생성된 데이터 확인.....\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_path\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m   \u001b[0mSentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"생성된 가사 수 : {} , 가사 시퀀스 수 : {} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-123-fc5cd9db386d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    226\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"생성된 데이터 확인.....\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_path\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m   \u001b[0mSentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"생성된 가사 수 : {} , 가사 시퀀스 수 : {} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N83BAw8lMd6i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}