{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "014_Lyric_Transformer_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tDtg4Tj8roG",
        "outputId": "629e5462-e36f-4ddc-8cf6-7e924e94abe1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr 21 09:31:48 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaDneNTR9Tm-"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS2EnRJG9flx",
        "outputId": "30b809a0-31af-47ac-fa93-b521c6567c44"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u55Os9VU854W",
        "outputId": "94d2298a-9d69-4731-8b65-a4091975be7a"
      },
      "source": [
        "!git clone https://github.com/Rnlcksgdkd/test_GAN"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'test_GAN' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "NSH9hxZ49I0w",
        "outputId": "73ae96ab-fb50-4785-8764-ead513a45258"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/Project/project_data/Ballard_Data.csv\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df = df.dropna(axis=0)\n",
        "df.info()\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1243 entries, 0 to 1242\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Unnamed: 0  1243 non-null   int64 \n",
            " 1   title       1243 non-null   object\n",
            " 2   singer      1243 non-null   object\n",
            " 3   lyrics      1243 non-null   object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 48.6+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>singer</th>\n",
              "      <th>lyrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>너희는 (Feat. 김광률)</td>\n",
              "      <td>이은영</td>\n",
              "      <td>너희는 하나님의 택하신\\n거룩하고 사랑스러운 자니 \\n긍율과 자비와 \\n겸손과 온유...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>봄날은 간다 (Bonus Track)</td>\n",
              "      <td>김윤아</td>\n",
              "      <td>눈을 감으면 문득 그리운 날의 기억 \\n아직까지도 마음이 저려 오는 건 \\n\\n그건...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Going Home</td>\n",
              "      <td>김윤아</td>\n",
              "      <td>집으로 돌아가는 길에\\n지는 햇살에 마음을 맡기고\\n나는 너의 일을 떠올리며\\n수많...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>야상곡 (夜想曲)</td>\n",
              "      <td>김윤아</td>\n",
              "      <td>바람이 부는 것은 더운 내 맘 삭여주려 \\n계절이 다 가도록 나는 애만 태우네 \\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>길</td>\n",
              "      <td>김윤아</td>\n",
              "      <td>아무도 가르쳐 주지 않아\\n이 길이 옳은지 다른 길로 가야 할지\\n난 저길 저 끝에...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                             lyrics\n",
              "0           0  ...  너희는 하나님의 택하신\\n거룩하고 사랑스러운 자니 \\n긍율과 자비와 \\n겸손과 온유...\n",
              "1           1  ...  눈을 감으면 문득 그리운 날의 기억 \\n아직까지도 마음이 저려 오는 건 \\n\\n그건...\n",
              "2           2  ...  집으로 돌아가는 길에\\n지는 햇살에 마음을 맡기고\\n나는 너의 일을 떠올리며\\n수많...\n",
              "3           3  ...  바람이 부는 것은 더운 내 맘 삭여주려 \\n계절이 다 가도록 나는 애만 태우네 \\n...\n",
              "4           4  ...  아무도 가르쳐 주지 않아\\n이 길이 옳은지 다른 길로 가야 할지\\n난 저길 저 끝에...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "aYSymCYFwp6-",
        "outputId": "6a17a54a-b8b9-46a1-8bd1-3a997e1bd586"
      },
      "source": [
        "df.dropna(axis=0, inplace = True)\n",
        "df['lyrics'] = df['lyrics'].str.replace('[^가-힣\\n]' , ' ')\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>singer</th>\n",
              "      <th>lyrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>너희는 (Feat. 김광률)</td>\n",
              "      <td>이은영</td>\n",
              "      <td>너희는 하나님의 택하신\\n거룩하고 사랑스러운 자니 \\n긍율과 자비와 \\n겸손과 온유...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>봄날은 간다 (Bonus Track)</td>\n",
              "      <td>김윤아</td>\n",
              "      <td>눈을 감으면 문득 그리운 날의 기억 \\n아직까지도 마음이 저려 오는 건 \\n\\n그건...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Going Home</td>\n",
              "      <td>김윤아</td>\n",
              "      <td>집으로 돌아가는 길에\\n지는 햇살에 마음을 맡기고\\n나는 너의 일을 떠올리며\\n수많...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>야상곡 (夜想曲)</td>\n",
              "      <td>김윤아</td>\n",
              "      <td>바람이 부는 것은 더운 내 맘 삭여주려 \\n계절이 다 가도록 나는 애만 태우네 \\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>길</td>\n",
              "      <td>김윤아</td>\n",
              "      <td>아무도 가르쳐 주지 않아\\n이 길이 옳은지 다른 길로 가야 할지\\n난 저길 저 끝에...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                             lyrics\n",
              "0           0  ...  너희는 하나님의 택하신\\n거룩하고 사랑스러운 자니 \\n긍율과 자비와 \\n겸손과 온유...\n",
              "1           1  ...  눈을 감으면 문득 그리운 날의 기억 \\n아직까지도 마음이 저려 오는 건 \\n\\n그건...\n",
              "2           2  ...  집으로 돌아가는 길에\\n지는 햇살에 마음을 맡기고\\n나는 너의 일을 떠올리며\\n수많...\n",
              "3           3  ...  바람이 부는 것은 더운 내 맘 삭여주려 \\n계절이 다 가도록 나는 애만 태우네 \\n...\n",
              "4           4  ...  아무도 가르쳐 주지 않아\\n이 길이 옳은지 다른 길로 가야 할지\\n난 저길 저 끝에...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCSG9Eck9ul4",
        "outputId": "95988e7e-85cd-4ba3-9021-a5ee41793fe0"
      },
      "source": [
        "lyric_data = df['lyrics'].values.tolist()\n",
        "len(lyric_data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1243"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqSqVyrOwYtI"
      },
      "source": [
        "Lyrics = []\n",
        "for lyric in lyric_data :\n",
        "    lst = [token for token in lyric.split(\"\\n\") if token != \"\" and token != \" \"]\n",
        "    Lyrics.append(lst)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe_5tBYwwc3p"
      },
      "source": [
        "Lyrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsj02e2YwEkJ"
      },
      "source": [
        "## Bigram 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6_U3IjmeCQq"
      },
      "source": [
        "bigram_sentences = []\n",
        "for lyrics in Lyrics:\n",
        "  bigram = []\n",
        "  for i in range(len(lyrics)):\n",
        "    if i+1 >= len(lyrics): break\n",
        "    bigram.append(lyrics[i] + ' ' + lyrics[i+1])\n",
        "  bigram_sentences.append(bigram)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfx_jQrADp5y"
      },
      "source": [
        "bigram_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdcGR1TG-cdH"
      },
      "source": [
        "Lyric_input = []\n",
        "Lyric_output = []\n",
        "\n",
        "for lyric in bigram_sentences:\n",
        "  for i, words in enumerate(lyric):\n",
        "    if i + 1 < len(lyric):\n",
        "      Lyric_input.append(words)\n",
        "      Lyric_output.append(lyric[i + 1])\n",
        "    else:\n",
        "      break"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dX55xST-o1_",
        "outputId": "8013cbaf-c6ef-4136-a50d-74b499950634"
      },
      "source": [
        "for i in range(120 , 130):\n",
        "  print(Lyric_input[i] + \"///\" + Lyric_output[i])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "이 말 밖에 없는 한 번만 더 내 이름///한 번만 더 내 이름 불러 줄래\n",
            "한 번만 더 내 이름 불러 줄래///불러 줄래 한 번만 더 곁에\n",
            "불러 줄래 한 번만 더 곁에///한 번만 더 곁에 있어 줄래\n",
            "한 번만 더 곁에 있어 줄래///있어 줄래 사랑해 사랑한\n",
            "있어 줄래 사랑해 사랑한///사랑해 사랑한 내 사람\n",
            "사랑해 사랑한 내 사람///내 사람 눈뜨면 숨 쉬면\n",
            "내 사람 눈뜨면 숨 쉬면///눈뜨면 숨 쉬면 보고 싶은 볼 수 없는\n",
            "눈뜨면 숨 쉬면 보고 싶은 볼 수 없는///보고 싶은 볼 수 없는 시간이 멈춰버린 듯\n",
            "보고 싶은 볼 수 없는 시간이 멈춰버린 듯///시간이 멈춰버린 듯 내 아픈 사랑\n",
            "차가운 바람은 나를 찌르고 무너진 가슴을 더 아프게 해///무너진 가슴을 더 아프게 해 꽃잎은 떨어지고 이제는 다 시들어\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpGMQ1lm-sf5"
      },
      "source": [
        "## 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGOijRaO-qw4",
        "outputId": "708fcd79-2ac6-4d3b-bf5e-cd6471d5a784"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpzYsT_1-wKW"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "\n",
        "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "PAD = \"<PAD>\"\n",
        "STD = \"<SOS>\"\n",
        "END = \"<END>\"\n",
        "UNK = \"<UNK>\"\n",
        "\n",
        "PAD_INDEX = 0\n",
        "STD_INDEX = 1\n",
        "END_INDEX = 2\n",
        "UNK_INDEX = 3\n",
        "\n",
        "MARKER = [PAD, STD, END, UNK]\n",
        "CHANGE_FILTER = re.compile(FILTERS)\n",
        "\n",
        "MAX_SEQUENCE = 25\n",
        "\n",
        "\n",
        "def load_data(path):\n",
        "  # 판다스를 통해서 데이터를 불러온다.\n",
        "  data_df = pd.read_csv(path, header=0)\n",
        "  # 질문과 답변 열을 가져와 question과 answer에 넣는다.\n",
        "  question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "\n",
        "  return question, answer\n",
        "\n",
        "\n",
        "def data_tokenizer(data):\n",
        "  # 토크나이징 해서 담을 배열 생성\n",
        "  words = []\n",
        "  for sentence in data:\n",
        "      # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "      # 위 필터와 같은 값들을 정규화 표현식을\n",
        "      # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n",
        "      sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
        "      for word in sentence.split():\n",
        "          words.append(word)\n",
        "  # 토그나이징과 정규표현식을 통해 만들어진\n",
        "  # 값들을 넘겨 준다.\n",
        "  return [word for word in words if word]\n",
        "\n",
        "\n",
        "def prepro_like_morphlized(data):\n",
        "  morph_analyzer = Okt()\n",
        "  result_data = list()\n",
        "  for seq in tqdm(data):\n",
        "      morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
        "      result_data.append(morphlized_seq)\n",
        "\n",
        "  return result_data\n",
        "\n",
        "\n",
        "def load_vocabulary(input , output ,  vocab_path, tokenize_as_morph=False):\n",
        "\n",
        "    \n",
        "  question, answer = input , output\n",
        "\n",
        "  if tokenize_as_morph:  # 형태소에 따른 토크나이져 처리\n",
        "      question = prepro_like_morphlized(question)\n",
        "      answer = prepro_like_morphlized(answer)\n",
        "  data = []\n",
        "\n",
        "  data.extend(question)\n",
        "  data.extend(answer)\n",
        "\n",
        "  words = data_tokenizer(data)\n",
        "\n",
        "  words = list(set(words))\n",
        "\n",
        "  words[:0] = MARKER\n",
        "\n",
        "  with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:\n",
        "      for word in words:\n",
        "          vocabulary_file.write(word + '\\n')\n",
        "\n",
        "  vocabulary_list = []\n",
        "  with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:\n",
        "      for line in vocabulary_file:\n",
        "          vocabulary_list.append(line.strip())\n",
        "\n",
        "  # 배열에 내용을 키와 값이 있는\n",
        "  # 딕셔너리 구조로 만든다.\n",
        "  char2idx, idx2char = make_vocabulary(vocabulary_list)\n",
        "  # 두가지 형태의 키와 값이 있는 형태를 리턴한다.\n",
        "  # (예) 단어: 인덱스 , 인덱스: 단어)\n",
        "  return char2idx, idx2char, len(char2idx)     \n",
        "\n",
        "\n",
        "def make_vocabulary(vocabulary_list):\n",
        "  # 리스트를 키가 단어이고 값이 인덱스인\n",
        "  # 딕셔너리를 만든다.\n",
        "  char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n",
        "  # 리스트를 키가 인덱스이고 값이 단어인\n",
        "  # 딕셔너리를 만든다.\n",
        "  idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n",
        "  # 두개의 딕셔너리를 넘겨 준다.\n",
        "  return char2idx, idx2char\n",
        "\n",
        "\n",
        "def enc_processing(value, dictionary, tokenize_as_morph=False):\n",
        "  # 인덱스 값들을 가지고 있는\n",
        "  # 배열이다.(누적된다.)\n",
        "  sequences_input_index = []\n",
        "  # 하나의 인코딩 되는 문장의\n",
        "  # 길이를 가지고 있다.(누적된다.)\n",
        "  sequences_length = []\n",
        "  # 형태소 토크나이징 사용 유무\n",
        "  if tokenize_as_morph:\n",
        "      value = prepro_like_morphlized(value)\n",
        "\n",
        "  # 한줄씩 불어온다.\n",
        "  for sequence in value:\n",
        "      # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "      # 정규화를 사용하여 필터에 들어 있는\n",
        "      # 값들을 \"\" 으로 치환 한다.\n",
        "      sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "      # 하나의 문장을 인코딩 할때\n",
        "      # 가지고 있기 위한 배열이다.\n",
        "      sequence_index = []\n",
        "      # 문장을 스페이스 단위로\n",
        "      # 자르고 있다.\n",
        "      for word in sequence.split():\n",
        "          # 잘려진 단어들이 딕셔너리에 존재 하는지 보고\n",
        "          # 그 값을 가져와 sequence_index에 추가한다.\n",
        "          if dictionary.get(word) is not None:\n",
        "              sequence_index.extend([dictionary[word]])\n",
        "          # 잘려진 단어가 딕셔너리에 존재 하지 않는\n",
        "          # 경우 이므로 UNK(2)를 넣어 준다.\n",
        "          else:\n",
        "              sequence_index.extend([dictionary[UNK]])\n",
        "      # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "      if len(sequence_index) > MAX_SEQUENCE:\n",
        "          sequence_index = sequence_index[:MAX_SEQUENCE]\n",
        "      # 하나의 문장에 길이를 넣어주고 있다.\n",
        "      sequences_length.append(len(sequence_index))\n",
        "      # max_sequence_length보다 문장 길이가\n",
        "      # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "      sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "      # 인덱스화 되어 있는 값을\n",
        "      # sequences_input_index에 넣어 준다.\n",
        "      sequences_input_index.append(sequence_index)\n",
        "  # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "  # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
        "  # 사전 작업이다.\n",
        "  # 넘파이 배열에 인덱스화된 배열과\n",
        "  # 그 길이를 넘겨준다.\n",
        "  return np.asarray(sequences_input_index), sequences_length\n",
        "\n",
        "\n",
        "def dec_output_processing(value, dictionary, tokenize_as_morph=False):\n",
        "  # 인덱스 값들을 가지고 있는\n",
        "  # 배열이다.(누적된다)\n",
        "  sequences_output_index = []\n",
        "  # 하나의 디코딩 입력 되는 문장의\n",
        "  # 길이를 가지고 있다.(누적된다)\n",
        "  sequences_length = []\n",
        "  # 형태소 토크나이징 사용 유무\n",
        "  if tokenize_as_morph:\n",
        "      value = prepro_like_morphlized(value)\n",
        "  # 한줄씩 불어온다.\n",
        "  for sequence in value:\n",
        "      # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "      # 정규화를 사용하여 필터에 들어 있는\n",
        "      # 값들을 \"\" 으로 치환 한다.\n",
        "      sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "      # 하나의 문장을 디코딩 할때 가지고\n",
        "      # 있기 위한 배열이다.\n",
        "      sequence_index = []\n",
        "      # 디코딩 입력의 처음에는 START가 와야 하므로\n",
        "      # 그 값을 넣어 주고 시작한다.\n",
        "      # 문장에서 스페이스 단위별로 단어를 가져와서 딕셔너리의\n",
        "      # 값인 인덱스를 넣어 준다.\n",
        "      sequence_index = [dictionary[STD]] + [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
        "      # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "      if len(sequence_index) > MAX_SEQUENCE:\n",
        "          sequence_index = sequence_index[:MAX_SEQUENCE]\n",
        "      # 하나의 문장에 길이를 넣어주고 있다.\n",
        "      sequences_length.append(len(sequence_index))\n",
        "      # max_sequence_length보다 문장 길이가\n",
        "      # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "      sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "      # 인덱스화 되어 있는 값을\n",
        "      # sequences_output_index 넣어 준다.\n",
        "      sequences_output_index.append(sequence_index)\n",
        "  # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "  # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
        "  # 사전 작업이다.\n",
        "  # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
        "  return np.asarray(sequences_output_index), sequences_length\n",
        "\n",
        "\n",
        "def dec_target_processing(value, dictionary, tokenize_as_morph=False):\n",
        "  # 인덱스 값들을 가지고 있는\n",
        "  # 배열이다.(누적된다)\n",
        "  sequences_target_index = []\n",
        "  # 형태소 토크나이징 사용 유무\n",
        "  if tokenize_as_morph:\n",
        "      value = prepro_like_morphlized(value)\n",
        "  # 한줄씩 불어온다.\n",
        "  for sequence in value:\n",
        "      # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "      # 정규화를 사용하여 필터에 들어 있는\n",
        "      # 값들을 \"\" 으로 치환 한다.\n",
        "      sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "      # 문장에서 스페이스 단위별로 단어를 가져와서\n",
        "      # 딕셔너리의 값인 인덱스를 넣어 준다.\n",
        "      # 디코딩 출력의 마지막에 END를 넣어 준다.\n",
        "      sequence_index = [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
        "      # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "      # 그리고 END 토큰을 넣어 준다\n",
        "      if len(sequence_index) >= MAX_SEQUENCE:\n",
        "          sequence_index = sequence_index[:MAX_SEQUENCE - 1] + [dictionary[END]]\n",
        "      else:\n",
        "          sequence_index += [dictionary[END]]\n",
        "      # max_sequence_length보다 문장 길이가\n",
        "      # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "      sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "      # 인덱스화 되어 있는 값을\n",
        "      # sequences_target_index에 넣어 준다.\n",
        "      sequences_target_index.append(sequence_index)\n",
        "  # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "  # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n",
        "  # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
        "  return np.asarray(sequences_target_index)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCKUg98G-3Qs"
      },
      "source": [
        "VOCAB_PATH = '/content/vocabulary.txt'\n",
        "\n",
        "char2idx, idx2char, vocab_size = load_vocabulary(Lyric_input , Lyric_output , VOCAB_PATH, tokenize_as_morph=False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8vBVL6s-53q"
      },
      "source": [
        "index_inputs, input_seq_len = enc_processing(Lyric_input, char2idx, tokenize_as_morph=False)\n",
        "index_outputs, output_seq_len = dec_output_processing(Lyric_output, char2idx, tokenize_as_morph=False)\n",
        "index_targets = dec_target_processing(Lyric_output, char2idx, tokenize_as_morph=False)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM21y5M4-8az",
        "outputId": "b327045e-f3c4-4af0-bda9-d5b08fb74f01"
      },
      "source": [
        "index_inputs.shape , index_outputs.shape , index_targets.shape , vocab_size , len(char2idx)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((29270, 25), (29270, 25), (29270, 25), 20868, 20868)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXkAkuWn--Ya"
      },
      "source": [
        "data_configs = {}\n",
        "data_configs['char2idx'] = char2idx\n",
        "data_configs['idx2char'] = idx2char\n",
        "data_configs['vocab_size'] = vocab_size\n",
        "data_configs['pad_symbol'] = PAD\n",
        "data_configs['std_symbol'] = STD\n",
        "data_configs['end_symbol'] = END\n",
        "data_configs['unk_symbol'] = UNK\n",
        "\n",
        "DATA_IN_PATH = '/content/'\n",
        "TRAIN_INPUTS = 'train_inputs.npy'\n",
        "TRAIN_OUTPUTS = 'train_outputs.npy'\n",
        "TRAIN_TARGETS = 'train_targets.npy'\n",
        "DATA_CONFIGS = 'data_configs.json'\n",
        "\n",
        "np.save(open(DATA_IN_PATH + TRAIN_INPUTS, 'wb'), index_inputs)\n",
        "np.save(open(DATA_IN_PATH + TRAIN_OUTPUTS , 'wb'), index_outputs)\n",
        "np.save(open(DATA_IN_PATH + TRAIN_TARGETS , 'wb'), index_targets)\n",
        "\n",
        "json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dns9B3Kq_AdW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}