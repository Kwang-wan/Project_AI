{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextRank.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSvXvQ9b4tD_"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9txyMf3W5os-",
        "outputId": "27147800-3ec7-4434-9bc0-61afe255955c"
      },
      "source": [
        "!pip install newspaper3k"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 4.7MB/s \n",
            "\u001b[?25hCollecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/62/b6acd3129c5615b9860e670df07fd55b76175b63e6b7f68282c7cad38e9e/tldextract-3.1.0-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.2MB/s \n",
            "\u001b[?25hCollecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Collecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 23.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.10)\n",
            "Collecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2020.12.5)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp37-none-any.whl size=13538 sha256=e3c6050d2d8648de069af4fd3fb1aa9d095d77563fa9aab619904c7ec2fcd8f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp37-none-any.whl size=3358 sha256=61b3388f7337ffb640ea214d5fcd988a72891e6a42c6db060e44fbccdcd9aea4\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp37-none-any.whl size=7398406 sha256=d34dae3bc8cb9b09c72a78fc72ae678689986575bccc9e7ef7650cb5b74d4847\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6067 sha256=a7225061b8245a8f61eb138e78b470bf7fb9b61b75520f986ac1fde1d8a5309d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, cssselect, requests-file, tldextract, sgmllib3k, feedparser, feedfinder2, jieba3k, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.2 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YU-cfNbH6FVV",
        "outputId": "97ee6227-592e-412c-c39e-4f4c04bd6587"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 24.4MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 34.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: beautifulsoup4, colorama, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSyNniSy6Kwm"
      },
      "source": [
        "from newspaper import Article\n",
        "from konlpy.tag import Kkma\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S0Q08lQ6gC0"
      },
      "source": [
        "num_summary=3\n",
        "\n",
        "class SentenceTokenizer(object):\n",
        "    def __init__(self):\n",
        "        self.kkma = Kkma()\n",
        "        self.okt = Okt()\n",
        "        self.stopwords =['이', '있', '하', '것', '들', '그', '되', '수', '이', '보', '않',\n",
        "                         '없', '나', '사람', '주', '아니', '등', '같', '우리', '때', '년', '가', '한', '지', \n",
        "                         '대하', '오', '말', '일', '그렇', '위하', '때문', '그것', '두', '말하', '알', '그러나', '받', \n",
        "                         '못하', '일', '그런', '또', '문제', '더', '사회', '많', '그리고', '좋', '크', '따르', '중', '나오', \n",
        "                         '가지', '씨', '시키', '만들', '지금', '생각하', '그러', '속', '하나', '집', '살', '모르', '적', '월', '데', '자신', \n",
        "                         '안', '어떤', '내', '내', '경우', '명', '생각', '시간', '그녀', '다시', '이런', '앞', '보이', '번', '나', '다른', '어떻', '여자',\n",
        "                         '개', '전', '들', '사실', '이렇', '점', '싶', '말', '정도', '좀', '원', '잘', '통하', '소리', '놓']\n",
        "    \n",
        "    def url2sentences(self, url):\n",
        "        article = Article(url, language='ko')\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        sentences = self.kkma.sentences(article.text)\n",
        "        \n",
        "        for idx in range(0, len(sentences)):\n",
        "            if len(sentences[idx]) <= 10:\n",
        "                sentences[idx-1] += (' ' + sentences[idx])\n",
        "                sentences[idx] = ''        \n",
        "        return sentences\n",
        "  \n",
        "    def text2sentences(self, text):\n",
        "        sentences = self.kkma.sentences(text)      \n",
        "        for idx in range(0, len(sentences)):\n",
        "            if len(sentences[idx]) <= 10:\n",
        "                sentences[idx-1] += (' ' + sentences[idx])\n",
        "                sentences[idx] = ''\n",
        "        return sentences\n",
        "\n",
        "    def get_nouns(self, sentences):\n",
        "        nouns = []\n",
        "        for sentence in sentences:\n",
        "            if sentence is not '':\n",
        "                nouns.append(' '.join([noun for noun in self.okt.nouns(str(sentence)) \n",
        "                                       if noun not in self.stopwords and len(noun) > 1]))\n",
        "        return nouns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvTob76y8NdX"
      },
      "source": [
        "class GraphMatrix(object):\n",
        "    def __init__(self):\n",
        "        self.tfidf = TfidfVectorizer()\n",
        "        self.cnt_vec = CountVectorizer()\n",
        "        self.graph_sentence = []\n",
        "\n",
        "    def build_sent_graph(self, sentence):\n",
        "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
        "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
        "        return self.graph_sentence\n",
        "        \n",
        "    def build_words_graph(self, sentence):\n",
        "        cnt_vec_mat = normalize(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
        "        vocab = self.cnt_vec.vocabulary_\n",
        "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVWJjtPC8eol"
      },
      "source": [
        "class Rank(object):\n",
        "    def get_ranks(self, graph, d=0.85): \n",
        "        A = graph\n",
        "        matrix_size = A.shape[0]\n",
        "        for id in range(matrix_size):\n",
        "            A[id, id] = 0 \n",
        "            link_sum = np.sum(A[:,id])\n",
        "            if link_sum != 0:\n",
        "                A[:, id] /= link_sum\n",
        "            A[:, id] *= -d\n",
        "            A[id, id] = 1\n",
        "        B = (1-d) * np.ones((matrix_size, 1))\n",
        "        ranks = np.linalg.solve(A, B) \n",
        "        return {idx: r[0] for idx, r in enumerate(ranks)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82AAbYoN9CPG"
      },
      "source": [
        "class TextRank(object):\n",
        "    def __init__(self, text):\n",
        "        self.sent_tokenize = SentenceTokenizer()\n",
        "        \n",
        "        if text[:5] in ('http:', 'https'):\n",
        "            self.sentences = self.sent_tokenize.url2sentences(text)\n",
        "        else:\n",
        "            self.sentences = self.sent_tokenize.text2sentences(text)\n",
        "        \n",
        "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
        "        \n",
        "        self.graph_matrix = GraphMatrix()\n",
        "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
        "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
        "\n",
        "        self.rank = Rank()\n",
        "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
        "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
        "\n",
        "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
        "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
        "    \n",
        "    def summarize(self, sent_num=3):\n",
        "        summary = []\n",
        "        index=[]\n",
        "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
        "            index.append(idx)\n",
        "        index.sort()\n",
        "\n",
        "        for idx in index:\n",
        "            summary.append(self.sentences[idx])\n",
        "        return summary\n",
        "\n",
        "    def keywords(self, word_num=10):\n",
        "        rank = Rank()\n",
        "        rank_idx = rank.get_ranks(self.words_graph)\n",
        "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
        "\n",
        "        keywords = []\n",
        "        index=[]\n",
        "        \n",
        "        for idx in sorted_rank_idx[:word_num]:\n",
        "            index.append(idx)\n",
        "\n",
        "        #index.sort()\n",
        "        for idx in index:\n",
        "            keywords.append(self.idx2word[idx])\n",
        "\n",
        "        return keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsIUxl4j_xUA"
      },
      "source": [
        "url = 'https://news.v.daum.net/v/20210408153522187'\n",
        "textrank = TextRank(url)\n",
        "for row in textrank.summarize(3):\n",
        "    print(row)\n",
        "    print()\n",
        "print('keywords :',textrank.keywords())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Liabedp8MZkB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}